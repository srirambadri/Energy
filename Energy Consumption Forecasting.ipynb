{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "df = pd.read_csv(\"london_energy.csv\")\n",
    "print(df.isna().sum())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_consumption = df.groupby(\"Date\")[\"KWH\"].mean()\n",
    "df_avg_consumption = pd.DataFrame({\"date\": df_avg_consumption.index.tolist(), \"consumption\": df_avg_consumption.values.tolist()})\n",
    "df_avg_consumption[\"date\"] = pd.to_datetime(df_avg_consumption[\"date\"])\n",
    "print(f\"From: {df_avg_consumption['date'].min()}\")\n",
    "print(f\"To: {df_avg_consumption['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mip solver with the CBC backend.\n",
    "solver = pywraplp.Solver.CreateSolver(\"CBC\")\n",
    "\n",
    "inf = solver.infinity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tIndex = input[\"simData\"][\"tIndex\"] # number of timeslots\n",
    "dt = input[\"simData\"][\"dt\"] # time interval in hour\n",
    "\n",
    "# Create datetime array\n",
    "startTime = input[\"simData\"][\"startTime\"].strftime(\"%d/%m/%Y %H:%M\")\n",
    "tIndex = input[\"simData\"][\"tIndex\"]\n",
    "timestamp = pd.date_range(startTime, periods=tIndex, freq=str(dt * 60) + \"min\")\n",
    "time = [timestamp[i].strftime(\"%d/%m/%Y %H:%M\") for i in range(len(timestamp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_s = timeit.default_timer()\n",
    "# Add timeseries variables\n",
    "vGrid = [solver.NumVar(lb=-inf, ub=inf, name=\"\") for _ in range(tIndex)]\n",
    "\n",
    "vBattPower = [solver.NumVar(lb=-inf, ub=inf, name=\"\") for _ in range(tIndex)]\n",
    "vCharge = [solver.NumVar(lb=-inf, ub=0, name=\"\") for _ in range(tIndex)]\n",
    "vDischarge = [solver.NumVar(lb=0, ub=inf, name=\"\") for _ in range(tIndex)]\n",
    "vChargeStatus = [solver.BoolVar(name=\"\") for _ in range(tIndex)]\n",
    "vSOC = [solver.NumVar(lb=0, ub=1, name=\"\") for _ in range(tIndex)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_consumption.plot(x=\"date\", y=\"consumption\")\n",
    "df_avg_consumption.query(\"date > '2012-01-01' & date < '2013-01-01'\").plot(x=\"date\", y=\"consumption\")\n",
    "df_avg_consumption[\"day_of_week\"] = df_avg_consumption[\"date\"].dt.dayofweek\n",
    "df_avg_consumption[\"day_of_year\"] = df_avg_consumption[\"date\"].dt.dayofyear\n",
    "df_avg_consumption[\"month\"] = df_avg_consumption[\"date\"].dt.month\n",
    "df_avg_consumption[\"quarter\"] = df_avg_consumption[\"date\"].dt.quarter\n",
    "df_avg_consumption[\"year\"] = df_avg_consumption[\"date\"].dt.year\n",
    "\n",
    "df_avg_consumption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mask = df_avg_consumption[\"date\"] < \"2013-07-28\"\n",
    "training_data = df_avg_consumption.loc[training_mask]\n",
    "print(training_data.shape)\n",
    "\n",
    "testing_mask = df_avg_consumption[\"date\"] >= \"2013-07-28\"\n",
    "testing_data = df_avg_consumption.loc[testing_mask]\n",
    "print(testing_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(20, 5))\n",
    "training_data.plot(ax=ax, label=\"Training\", x=\"date\", y=\"consumption\")\n",
    "testing_data.plot(ax=ax, label=\"Testing\", x=\"date\", y=\"consumption\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary `date` column\n",
    "training_data = training_data.drop(columns=[\"date\"])\n",
    "testing_dates = testing_data[\"date\"]\n",
    "testing_data = testing_data.drop(columns=[\"date\"])\n",
    "\n",
    "X_train = training_data[[\"day_of_week\", \"day_of_year\", \"month\", \"quarter\", \"year\"]]\n",
    "y_train = training_data[\"consumption\"]\n",
    "\n",
    "X_test = testing_data[[\"day_of_week\", \"day_of_year\", \"month\", \"quarter\", \"year\"]]\n",
    "y_test = testing_data[\"consumption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, prediction):\n",
    "  print(f\"MAE: {mean_absolute_error(y_test, prediction)}\")\n",
    "  print(f\"MSE: {mean_squared_error(y_test, prediction)}\")\n",
    "  print(f\"MAPE: {mean_absolute_percentage_error(y_test, prediction)}\")\n",
    "\n",
    "def plot_predictions(testing_dates, y_test, prediction):\n",
    "  df_test = pd.DataFrame({\"date\": testing_dates, \"actual\": y_test, \"prediction\": prediction })\n",
    "  figure, ax = plt.subplots(figsize=(10, 5))\n",
    "  df_test.plot(ax=ax, label=\"Actual\", x=\"date\", y=\"actual\")\n",
    "  df_test.plot(ax=ax, label=\"Prediction\", x=\"date\", y=\"prediction\")\n",
    "  plt.legend([\"Actual\", \"Prediction\"])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating GridSearch results\n",
    "prediction = grid_search.predict(X_test)\n",
    "plot_predictions(testing_dates, y_test, prediction)\n",
    "evaluate_model(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = pd.read_csv(\"london_weather.csv\")\n",
    "print(df_weather.isna().sum())\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing dates\n",
    "df_weather[\"date\"] = pd.to_datetime(df_weather[\"date\"], format=\"%Y%m%d\")\n",
    "\n",
    "# Filling missing values through interpolation\n",
    "df_weather = df_weather.interpolate(method=\"ffill\")\n",
    "\n",
    "# Enhancing consumption dataset with weather information\n",
    "df_avg_consumption = df_avg_consumption.merge(df_weather, how=\"inner\", on=\"date\")\n",
    "df_avg_consumption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary `date` column\n",
    "training_data = training_data.drop(columns=[\"date\"])\n",
    "testing_dates = testing_data[\"date\"]\n",
    "testing_data = testing_data.drop(columns=[\"date\"])\n",
    "\n",
    "X_train = training_data[[\"day_of_week\", \"day_of_year\", \"month\", \"quarter\", \"year\",\\\n",
    "                         \"cloud_cover\", \"sunshine\", \"global_radiation\", \"max_temp\",\\\n",
    "                         \"mean_temp\", \"min_temp\", \"precipitation\", \"pressure\",\\\n",
    "                         \"snow_depth\"]]\n",
    "y_train = training_data[\"consumption\"]\n",
    "\n",
    "\n",
    "X_test = testing_data[[\"day_of_week\", \"day_of_year\", \"month\", \"quarter\", \"year\",\\\n",
    "                         \"cloud_cover\", \"sunshine\", \"global_radiation\", \"max_temp\",\\\n",
    "                         \"mean_temp\", \"min_temp\", \"precipitation\", \"pressure\",\\\n",
    "                         \"snow_depth\"]]\n",
    "y_test = testing_data[\"consumption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "cv_split = TimeSeriesSplit(n_splits=4, test_size=100)\n",
    "model = XGBRegressor()\n",
    "parameters = {\n",
    "    \"max_depth\": [3, 4, 6, 5, 10],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    \"n_estimators\": [100, 300, 500, 700, 900, 1000],\n",
    "    \"colsample_bytree\": [0.3, 0.5, 0.7]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM\n",
    "cv_split = TimeSeriesSplit(n_splits=4, test_size=100)\n",
    "model = lgb.LGBMRegressor()\n",
    "parameters = {\n",
    "    \"max_depth\": [3, 4, 6, 5, 10],\n",
    "    \"num_leaves\": [10, 20, 30, 40, 100, 120],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    \"n_estimators\": [50, 100, 300, 500, 700, 900, 1000],\n",
    "    \"colsample_bytree\": [0.3, 0.5, 0.7, 1]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
